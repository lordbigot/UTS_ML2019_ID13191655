{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordbigot/UTS_ML2019_ID13191655/blob/master/A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCraNF53VnoF",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/19GO4vG1Df6jkWbBOs9lrxm8SWhY6AJ56\n",
        "\n",
        "https://github.com/lordbigot/UTS_ML2019_ID13191655/blob/master/A3.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFpomFbE58q",
        "colab_type": "text"
      },
      "source": [
        "# The Multi-armed Bandit Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDc4uLtzFc_A",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCQLq53mFIEM",
        "colab_type": "text"
      },
      "source": [
        "Consider a machine with three arms. When the first arm is pulled, the machine returns \\$5. When the second arm is pulled, the machine returns nothing. When the third arm is pulled, the machine has a 10% chance of returning \\$1000 dollars. If the arms are pulled indefinitely, the third arm will produce the greatest reward. However, an observer can only learn the nature of this machine by pulling its arms. To get a complete picture, they are likely to start by pulling each arm once. In this case, there is a 90% chance that they will recieve no money from the third arm. When this occurs, they cannot distinguish between the second and third arms. From their perspective, it is possible neither arm will produce any reward.\n",
        "\n",
        "This thought experiment brings me to a preliminary algorithm. First, each arm is pulled once. Then, the arms are sorted by their overall output. After this point, for as long as the machine can continue running, the arm with the greatest average output is pulled until the number of times it has been pulled exceeds n^2, where n is the number of times the arm with the lowest average output has been pulled. Each other arm is pulled, in series of highest to lowest average, until the number of times it has been pulled exceeds n. This algorithm will quickly scale, pulling the best-performing lever frequently, but continuing to pull the other levers as the risk persists that an otherwise poor-performing lever is the optimal solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DW3a_KqF1aE",
        "colab_type": "text"
      },
      "source": [
        "## First steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7e9ymM_YFhT",
        "colab_type": "text"
      },
      "source": [
        "Consider a machine with 12 levers. You will only be able to pull a lever a number of times between 2 and 12. You are unaware of how many pulls you will recieve. Assume, each lever returns a constant reward, but the expected reward is unknown to you. In this case, it is apparent that attempting to pull as many unknown levers as possible will produce middling results, as will pulling one lever as many times as possible. An optimal solution may be to pull k levers, and then pull the lever that produced the highest reward. For all number of pulls n between 2 and 12, the ideal number for k sits between 2 and (n+1)/2 inclusive. If the distribution of lever rewards across the machine has a strong left skew, pulling the lever twice is ideal, as it is where the lowest values of lever are least commonly pulled. This is because there are the least chances of testing a low-valued lever, but if one is found, another lever can be used. Keep in mind that the algorithm has no way of evaluating if the reward is below normal until a second lever is pulled. If the distribution has a strong right skew, pulling the lever (n+1)/2 times is ideal, as it is where the highest values are most commonly pulled. This is because the best product from splitting a number into two numbers and then multiplying them together is a square. If n = 8, and k = 5, there are 5 chances of pulling the best lever, and if it is found, it will be pulled 3 more times, for a total of 4 times, and a product of 20 times across all possible cases. If n = 8, and k = 7, there are 7 chances of pulling the best lever. However, if it is found, it will be only be pulled one more time, for a total of 2 times. This produces the comparably poor product of 14 pulls across all possible cases. An algorithm to identify the proportions of possible pulls is included below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvPuvwCdYDDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_permutations(n, target_index, current_index):\n",
        "    if current_index == 0:\n",
        "        new_list = []\n",
        "        for i in range(n):\n",
        "            new_list.append([i])\n",
        "        return new_list\n",
        "    list_below = list_permutations(n, target_index, current_index - 1)\n",
        "    new_list = []\n",
        "    for row in list_below:\n",
        "        new_elements = []\n",
        "        for i in range(n):\n",
        "            found_element = False\n",
        "            for element in row:\n",
        "                found_element = found_element or element == i\n",
        "            if not found_element:\n",
        "                new_elements.append([i])\n",
        "        for element in new_elements:\n",
        "            new_list.append(row + element)\n",
        "    return new_list\n",
        "\n",
        "\n",
        "def complete_list(n, data):\n",
        "    missing_rows = n - len(data[0])\n",
        "    for row in data:\n",
        "        maximum_class = 0\n",
        "        for element in row:\n",
        "            maximum_class = max(maximum_class, element)\n",
        "        for i in range(missing_rows):\n",
        "            row.append(maximum_class)\n",
        "    return data\n",
        "\n",
        "\n",
        "def frequency_analysis(n, data):\n",
        "    new_list = []\n",
        "    for i in range(n):\n",
        "        new_list.append(0)\n",
        "    for row in data:\n",
        "        for element in row:\n",
        "            new_list[element] += 1\n",
        "    divisor = len(data) * n\n",
        "    for i in range(n):\n",
        "        new_list[i] /= divisor\n",
        "    return new_list\n",
        "\n",
        "\n",
        "def frequency_comparison(n):\n",
        "    print(\"n:\", n)\n",
        "    for i in range(n):\n",
        "        print(\"k:\", i+1)\n",
        "        print(frequency_analysis(n, complete_list(n, list_permutations(n, i, i)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMWn1iRyYUPc",
        "colab_type": "text"
      },
      "source": [
        "Referring to our earlier problem, for all cases 3 <= n <= 12; k = 2 is a good solution. However, for the majority of values, k = 3 is likely to be a more rounded solution, less dependent on the assumption of a left skew. In this thought experiment, nothing is gained from pulling an apparently good lever before a new one is discovered. However, this will not always be the case. If we now assume that each lever has a variable result, then it may be useful to gather additional information on apparently good levers before moving onto other levers. In the short-term, this may increase overall output, and in the long-term, it will be insignificant, justified by the increased certainty of the effect of the lever. \n",
        "\n",
        "After examining this experiment, I am drawn to amend my earlier algorithm. This algorithm now has two processes. The overall process will pull two new levers, call the subprocess three times, pull a new lever, call the subprocess three times, pull another lever, and so on until it runs out new levers, at which point it continually calls the subprocess. The subprocess, when called, will sort the known levers from greatest average output to lowest average output. It will pull the first lever if the number of times it has been pulled is less than (x+1)^2, where x is the number of times the last lever has been pulled. Otherwise, it will pull the next following lever that has been pulled less than x+1 times. This algorithm will effectively produce the same results as the last algorithm if every lever can be pulled, but will produce above-average results faster than previously possible. This algorithm is encoded below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5AyLZPLYXRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition_levers(levers, output_sums, output_instances, low, high):\n",
        "    i = (low - 1)\n",
        "    pivot = output_sums[high] / output_instances[high]\n",
        "    for j in range(low, high):\n",
        "        if output_sums[j] / output_instances[j] >= pivot:\n",
        "            i += 1\n",
        "            levers[i], levers[j] = levers[j], levers[i]\n",
        "            output_sums[i], output_sums[j] = output_sums[j], output_sums[i]\n",
        "            output_instances[i], output_instances[j] = output_instances[j], output_instances[i]\n",
        "    levers[i+1], levers[high] = levers[high], levers[i+1]\n",
        "    output_sums[i+1], output_sums[high] = output_sums[high], output_sums[i+1]\n",
        "    output_instances[i+1], output_instances[high] = output_instances[high], output_instances[i+1]\n",
        "    return (i+1)\n",
        "\n",
        "\n",
        "def quick_lever_sort(levers, output_sums, output_instances, low, high):\n",
        "    if low < high:\n",
        "        pi = partition_levers(levers, output_sums, output_instances, low, high)\n",
        "        quick_lever_sort(levers, output_sums, output_instances, low, pi-1)\n",
        "        quick_lever_sort(levers, output_sums, output_instances, pi+1, high)\n",
        "\n",
        "\n",
        "def pull_known_lever(levers, output_sums, output_instances):\n",
        "    quick_lever_sort(levers, output_sums, output_instances, 0, len(output_sums) - 1)\n",
        "    x = output_instances[len(output_instances) - 1]\n",
        "    if output_instances[0] < pow(x + 1, 2):\n",
        "        output_sums[0] += pull_lever(levers[0])\n",
        "        output_instances[0] += 1\n",
        "        return\n",
        "    current_lever = 1\n",
        "    while current_lever < len(output_sums):\n",
        "        if output_instances[current_lever] < x + 1:\n",
        "            output_sums[current_lever] += pull_lever(levers[current_lever])\n",
        "            output_instances[current_lever] += 1\n",
        "            return\n",
        "        current_lever += 1\n",
        "\n",
        "\n",
        "def algorithm(levers):\n",
        "    output_sums = []\n",
        "    output_instances = []\n",
        "    output_sums.append(pull_lever(levers[0]))\n",
        "    output_instances.append(1)\n",
        "    current_lever = 1\n",
        "    while current_lever < len(levers):\n",
        "        output_sums.append(pull_lever(levers[current_lever]))\n",
        "        output_instances.append(1)\n",
        "        for i in range(3):\n",
        "            pull_known_lever(levers, output_sums, output_instances)\n",
        "        current_lever += 1\n",
        "    for i in range(100):\n",
        "        pull_known_lever(levers, output_sums, output_instances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLYx_l2qGIVj",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm Specialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vWVZ_fBYbj6",
        "colab_type": "text"
      },
      "source": [
        "This problem is well known in probability theory as the multi-armed bandit problem. However, most solutions assume four conditions that we are not given:\n",
        "\n",
        "(1) The results from each lever are based on a continuous probability curve.\n",
        "\n",
        "(2) The curve is without a significant skew.\n",
        "\n",
        "(3) The number of arms is small.\n",
        "\n",
        "(4) The amount of time it takes to test every arm is not worth considering, in comparison to the benefit of a preliminary examination of all variables.\n",
        "\n",
        "These assumptions are useful, but not comprehensive. If I wish to address the problem I have been given, then solutions that make broad assumptions are undesirable. However, there is information that can be learned. A generic rule, Hoeffding's Inequality, can be used as a representation of all curves. It is a calculation for the maximum distance the true mean can vary from the real mean, within a certain confidence level, given the number of samples. By rearranging this equation, we can calculate a 95% confidence level around each lever. If the upper bound of this region is less than the lower bound of the best lever, than the current lever is no longer worth investigation. A solution to check the adjusted formula is included below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezrN_ZRPYdCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hoeffding_maximum_confident_displacement(number_of_samples, given_range):\n",
        "    return 1.9206455826 * given_range / math.sqrt(number_of_samples)\n",
        "\n",
        "\n",
        "def hoeffding_comparison(high_number_of_samples, high_mean, low_number_of_samples, low_mean, given_range):\n",
        "    return low_mean + hoeffding_maximum_confident_displacement(low_number_of_samples, given_range) > high_mean - hoeffding_maximum_confident_displacement(high_number_of_samples, given_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE3mcgxUGRzG",
        "colab_type": "text"
      },
      "source": [
        "## Standards and Finalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbKHwyH6YfrF",
        "colab_type": "text"
      },
      "source": [
        "An important standard used in the evaluation of solutions to the multi-armed bandit problem is cumulative regret. Cumulative regret is the difference between the estimated mean of the best lever and the estimated mean of each other lever, multiplied by the times the other lever is used.\n",
        "\n",
        "However, it produces an incomplete evaluation. Consider an algorithm that picked one lever and pulled it indefinitely. It would never gain enough information to regret a decision. However, it would be unlikely to select the optimal lever. If every lever gets pulled only once, the regret increases, and the chance of settling upon the optimal lever increases. The challenge is to minimise regret and maximise chance of finding the ideal lever.\n",
        "\n",
        "As a seperate factor, a balance should be made between long term and short term cumulative regret. For good long term results, the best test that can be done is trying each lever in order, until the lever is confirmed to be no longer useful. For better short term results, testing of levers unlikely to return high values should be delayed, or stopped entirely.\n",
        "\n",
        "Now that the components have been isolated, it may be worth re-examining some variables. My earlier rate of testing 1 new lever every 4 pulls was based on the assumption that the array was not skewed. However, given that we are testing gambling machines, the existence of a right skew is likely, as the existence of high jackpots is often used. As a result, I will change the rate to 1 in 3 pulls. In addition, this may decrease the long term cumulative regret, as good levers later in the set can be found before a significant investment is made in other, earlier levers.\n",
        "\n",
        "Another factor worth reevaluating is the scaling proportions of the best and worst levers. The existing practice of the algorithm, using squares to ensure that pulling of lesser levers drops off, has drawbacks. To minimise long-term regret, the algorithm would function best by pulling each lever in uniform proportions. To minimise short-term regret, the algorithm would function best by pulling the favored lever repeatedly. To compromise, and perform well uniformly, it should make many pulls of the best performing lever, but also pull other, milder performing levers. In an attempt to be as fair as possible, I will build it so that it maintains a constant ratio of pulling lever n to pulling lever n+1, where the levers are ordered.\n",
        "\n",
        "Factoring all of this into consideration, I have produced the following algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSM2HS1-_una",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def partition_levers(levers, output_sums, output_instances, low, high):\n",
        "    i = (low - 1)\n",
        "    pivot = output_sums[high] / output_instances[high]\n",
        "    for j in range(low, high):\n",
        "        if output_sums[j] / output_instances[j] >= pivot:\n",
        "            i += 1\n",
        "            levers[i], levers[j] = levers[j], levers[i]\n",
        "            output_sums[i], output_sums[j] = output_sums[j], output_sums[i]\n",
        "            output_instances[i], output_instances[j] = output_instances[j], output_instances[i]\n",
        "    levers[i+1], levers[high] = levers[high], levers[i+1]\n",
        "    output_sums[i+1], output_sums[high] = output_sums[high], output_sums[i+1]\n",
        "    output_instances[i+1], output_instances[high] = output_instances[high], output_instances[i+1]\n",
        "    return (i+1)\n",
        "\n",
        "\n",
        "def quick_lever_sort(levers, output_sums, output_instances, low, high):\n",
        "    if low < high:\n",
        "        pi = partition_levers(levers, output_sums, output_instances, low, high)\n",
        "        quick_lever_sort(levers, output_sums, output_instances, low, pi-1)\n",
        "        quick_lever_sort(levers, output_sums, output_instances, pi+1, high)\n",
        "        \n",
        "        \n",
        "def hoeffding_maximum_confident_displacement(number_of_samples, given_range):\n",
        "    return 1.9206455826 * given_range / math.sqrt(number_of_samples)\n",
        "\n",
        "\n",
        "def hoeffding_comparison(high_number_of_samples, high_mean, low_number_of_samples, low_mean, given_range):\n",
        "    return low_mean + hoeffding_maximum_confident_displacement(low_number_of_samples, given_range) > high_mean - hoeffding_maximum_confident_displacement(high_number_of_samples, given_range)\n",
        "\n",
        "\n",
        "def pull_known_lever(levers, output_sums, output_instances, given_range):\n",
        "    quick_lever_sort(levers, output_sums, output_instances, 0, len(output_sums) - 1)\n",
        "    known_levers = len(output_instances)\n",
        "    minimum_lever_exponential_base = pow(output_instances[0], 1 / (2.0*(known_levers-1)))\n",
        "    minimum_lever_exponential_base_index = 0\n",
        "    for i in range(1, known_levers):\n",
        "        if hoeffding_comparison(output_instances[0], output_sums[0] / output_instances[0], output_instances[i], output_sums[i] / output_instances[i], given_range):\n",
        "            lever_exponential_base = pow(output_instances[i], 1 / (2.0*(known_levers-1)-i))\n",
        "            if lever_exponential_base < minimum_lever_exponential_base:\n",
        "                minimum_lever_exponential_base = lever_exponential_base\n",
        "                minimum_lever_exponential_base_index = i\n",
        "    result = pull_lever(levers[minimum_lever_exponential_base_index])\n",
        "    output_sums[minimum_lever_exponential_base_index] += pull_lever(levers[minimum_lever_exponential_base_index])\n",
        "    output_instances[minimum_lever_exponential_base_index] += 1\n",
        "    return result\n",
        "\n",
        "\n",
        "def algorithm(levers, pulls):\n",
        "    output_sums = []\n",
        "    output_instances = []\n",
        "    minimum_value = pull_lever(levers[0])\n",
        "    maximum_value = minimum_value\n",
        "    intermediate_range = 0.001\n",
        "    output_sums.append(minimum_value)\n",
        "    output_instances.append(1)\n",
        "    count = 1\n",
        "    current_lever = 1\n",
        "    while current_lever < len(levers):\n",
        "        recent_result = pull_lever(levers[current_lever])\n",
        "        count += 1\n",
        "        if recent_result < minimum_value:\n",
        "            minimum_value = recent_result\n",
        "            intermediate_range = maximum_value - minimum_value\n",
        "        elif recent_result > maximum_value:\n",
        "            maximum_value = recent_result\n",
        "            intermediate_range = maximum_value - minimum_value\n",
        "        output_sums.append(recent_result)\n",
        "        output_instances.append(1)\n",
        "        for i in range(2):\n",
        "            recent_result = pull_known_lever(levers, output_sums, output_instances, intermediate_range)\n",
        "            count += 1\n",
        "            if recent_result < minimum_value:\n",
        "                minimum_value = recent_result\n",
        "                intermediate_range = maximum_value - minimum_value\n",
        "            elif recent_result > maximum_value:\n",
        "                maximum_value = recent_result\n",
        "                intermediate_range = maximum_value - minimum_value\n",
        "        current_lever += 1\n",
        "    for i in range(count, pulls):\n",
        "        recent_result = pull_known_lever(levers, output_sums, output_instances, intermediate_range)\n",
        "        if recent_result < minimum_value:\n",
        "            minimum_value = recent_result\n",
        "            intermediate_range = maximum_value - minimum_value\n",
        "        elif recent_result > maximum_value:\n",
        "            maximum_value = recent_result\n",
        "            intermediate_range = maximum_value - minimum_value\n",
        "    result_sum = 0\n",
        "    for current_sum in output_sums:\n",
        "        result_sum += current_sum\n",
        "    return result_sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrMXXxEpGX-A",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLWCHhKbFPzj",
        "colab_type": "text"
      },
      "source": [
        "For the purposes of testing, a comprehensive lever capable of replicating a variety of test conditions is desirable.\n",
        "\n",
        "The below types of levers are:\n",
        "\n",
        "0: The lever has a given chance of returning 1, otherwise returns 0. This is a special case of the following lever\n",
        "\n",
        "1: The lever has a given chance of returning a given value, a given chance of returning another value, and so on\n",
        "\n",
        "2: The lever returns values along to a gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeZqByaFEVZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def pull_lever(lever):\n",
        "  if lever[0] == 0:\n",
        "    if random.random() < lever[1]:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  elif lever[0] == 1:\n",
        "    index = 1\n",
        "    key = random.random()\n",
        "    while index + 1 < len(lever):\n",
        "      if key < lever[index]:\n",
        "        return lever[index + 1]\n",
        "      index += 2\n",
        "    return lever[index]\n",
        "  elif lever[0] == 2:\n",
        "    return random.gauss(lever[1], lever[2])\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnt9vEGuFkRF",
        "colab_type": "text"
      },
      "source": [
        "As an initial test, I will try using a series of 10 levers, each with a different chance of returning a consistent reward, from 0% to 90%.\n",
        "\n",
        "100 pulls is a good middle-of-the-range number, where information is gathered, but not\n",
        "\n",
        "Given 100 pulls, a solution with perfect information would on average produce 90 points. A solution with no information would on average produce 45 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hCFoOmuGjCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e4ad8e2c-1a39-4094-93be-794a2dc20295"
      },
      "source": [
        "test_length = 100\n",
        "number_of_pulls = 10000\n",
        "\n",
        "# in order\n",
        "point_sum = 0\n",
        "for i in range(test_length):\n",
        "    levers = [[0,0.0],[0,0.1],[0,0.2],[0,0.3],[0,0.4],[0,0.5],[0,0.6],[0,0.7],[0,0.8],[0,0.9]]\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_length)\n",
        "\n",
        "# in reverse order\n",
        "point_sum = 0\n",
        "for i in range(test_length):\n",
        "    levers = [[0,0.9],[0,0.8],[0,0.7],[0,0.6],[0,0.5],[0,0.4],[0,0.3],[0,0.2],[0,0.1],[0,0.0]]\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_length)\n",
        "\n",
        "# in unambiguous order\n",
        "point_sum = 0\n",
        "for i in range(test_length):\n",
        "    levers = [[0,0.9],[0,0.0],[0,0.1],[0,0.2],[0,0.3],[0,0.4],[0,0.5],[0,0.6],[0,0.7],[0,0.8]]\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_length)\n",
        "\n",
        "# in random orders\n",
        "point_sum = 0\n",
        "for i in range(test_length):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_length)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8705.77\n",
            "8695.66\n",
            "8704.76\n",
            "8702.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZJEzQ_pJPep",
        "colab_type": "text"
      },
      "source": [
        "I was able to accumulate the following averages, given 100 lever pulls:\n",
        "\n",
        "in order: 64.3968\n",
        "\n",
        "in reverse order: 64.2870\n",
        "\n",
        "in unambiguous order: 64.3507\n",
        "\n",
        "in random order: 64.3114\n",
        "\n",
        "This demonstrates that the grouping of levers is mostly insignificant in the overall outcome, which is definitively influenced by the information gathered.\n",
        "\n",
        "In addition, I have data on the cumulative regret of other algorithms performing the same test.\n",
        "\n",
        "If this algorithm gained perfect information, its cumulative regret at 2500 time steps would be equal to 2250 minus the average point total. This is 1992.94, leaving the regret at 247.83, worse than UCB1 (Weng 2018). The regret at 10000 time steps is 302.34, better than UCB1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0fl-X_-UgX5",
        "colab_type": "text"
      },
      "source": [
        "As a second test, I will use a massive jackpot, occurring at only 20% frequency, and compare it to a constant reward function.\n",
        "\n",
        "In each case:\n",
        "\n",
        "a solution with perfect information should have an average return of 20000\n",
        "\n",
        "a solution favoring the wrong choice should have an average return of 10000\n",
        "\n",
        "a solution with no information (ignoring decoys), should have an average return of 15000\n",
        "\n",
        "a solution with no information (including decoys), should have an average return of 3000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7SVq_a4XUK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c5065695-7b88-4688-98a3-74dd3c78acea"
      },
      "source": [
        "test_trials = 10000\n",
        "number_of_pulls = 100\n",
        "\n",
        "# jackpot is better than constant\n",
        "levers = [[1,1.0,100],[1,0.2,1000,0]]\n",
        "point_sum = 0\n",
        "for i in range(test_trials):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_trials)\n",
        "\n",
        "# jackpot is better than constant, plus decoys\n",
        "levers = [[1,1.0,100],[1,0.2,1000,0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0]]\n",
        "point_sum = 0\n",
        "for i in range(test_trials):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_trials)\n",
        "\n",
        "# constant is better than jackpot\n",
        "levers = [[1,1.0,200],[1,0.2,500,0]]\n",
        "point_sum = 0\n",
        "for i in range(test_trials):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_trials)\n",
        "\n",
        "# constant is better than jackpot, plus decoys\n",
        "levers = [[1,1.0,200],[1,0.2,500,0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0],[0,0.0]]\n",
        "point_sum = 0\n",
        "for i in range(test_trials):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_trials)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16387.6\n",
            "4196.9779\n",
            "17415.44\n",
            "4615.6946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqLC4l34d-2f",
        "colab_type": "text"
      },
      "source": [
        "I found the following averages under 100 pulls:\n",
        "\n",
        "Jackpot is better than constant: 16315.06\n",
        "\n",
        "Jackpot is better than constant (plus decoys): 4192.5923\n",
        "\n",
        "Constant is better than the jackpot: 17415.44\n",
        "\n",
        "Constant is better than jackpot (plus decoys): 4615.6946\n",
        "\n",
        "However, under the conditions, 100 pulls may not have given the algorithm enough information to demonstrate the capabilities of the function, so I also tested the same scenario on 1000 pulls, and divided the results by 10 (for the purposes of direct comparison)\n",
        "\n",
        "Jackpot is better than constant: 18269.384\n",
        "\n",
        "Jackpot is better than constant (plus decoys): 7154.03009\n",
        "\n",
        "Constant is better than jackpot: 18529.368\n",
        "\n",
        "Constant is better than jackpot (plus decoys): 9674.27925"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXT4MM4Ym-v4",
        "colab_type": "text"
      },
      "source": [
        "Having proven the efficacy of this algorithm with discrete values, I would like to confirm that it is simultaneously suitable for continuous functions. I will test a series of continuous and discrete levers.\n",
        "\n",
        "The best case scenario for this algorithm is 10000.\n",
        "If the algorithm does not work, the result will be closer to 6767."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw2XnRCbnydX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "950ee771-4eba-49d5-d6ae-c90b1db1bb02"
      },
      "source": [
        "test_trials = 1000\n",
        "number_of_pulls = 100\n",
        "\n",
        "# continuous is better than discrete\n",
        "levers = [[2,100,10],[2,70,100],[2,70,1],[1,0.5,150,0],[1,0.1,100,0.5,110,0.9,90,0],[0,1.0]]\n",
        "point_sum = 0\n",
        "for i in range(test_trials):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_trials)\n",
        "\n",
        "# discrete is better than continuous\n",
        "levers = [[2,75,10],[2,70,100],[2,70,1],[1,0.5,200,0],[1,0.1,100,0.5,110,0.9,90,0],[0,1.0]]\n",
        "point_sum = 0\n",
        "for i in range(test_trials):\n",
        "    random.shuffle(levers)\n",
        "    point_sum += algorithm(levers,number_of_pulls)\n",
        "print(point_sum/test_trials)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7816.88411779411\n",
            "7750.310547243608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PORytfGyaX",
        "colab_type": "text"
      },
      "source": [
        "The results in 100 pulls are approximately:\n",
        "\n",
        "continuous normal curve is best: 7816.88\n",
        "\n",
        "discrete dichotomy is best: 7750.310547243608"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbKfM-n5IIME",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLYEsNF2IMD2",
        "colab_type": "text"
      },
      "source": [
        "In summary, The Multi-armed bandit has a lot of factors, and a lot of possible solutions.\n",
        "\n",
        "Some solutions are specialised for particular distributions, and the majority are optimised for maximising output at high values. The standard approach to the exploration/exploitation trade-off favors exploration at low values, and expoitation at high values.\n",
        "\n",
        "I believe the algorithm I have produced is genuinely good for very small numbers of lever pulls, despite the compromise in early information gain. However, if I was to look for a true algorithm for every circumstance, I would likely pick a form of Thompson Sampling (Chapelle et al. 2011). Amongst other options, it makes a comparatively small number of assumptions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F-qn72wOXOp",
        "colab_type": "text"
      },
      "source": [
        "## Bibliography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bEHVh2BObCT",
        "colab_type": "text"
      },
      "source": [
        "Weng, L. 2018, 'The Multi-Armed Bandit Problem and Its Solutions', *Lil'Log*, weblog, github, Indian Ocean, 23 January, viewed 8 October 2019, <https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html>.\n",
        "\n",
        "Chapelle, O., & Lihong L. 2011, 'An empirical evaluation of thompson sampling.', *Advances in neural information processing systems*\n",
        "\n",
        "Martín, M., Jiménez-Martín A., & Mateos A. 2018, 'Possibilistic reward methods for the multi-armed bandit problem'\n",
        "*Neurocomputing*, vol. 310, pp. 201-212."
      ]
    }
  ]
}