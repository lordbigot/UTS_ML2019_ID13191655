{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordbigot/UTS_ML2019_ID13191655/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMCFXtnBvs2s",
        "colab_type": "text"
      },
      "source": [
        "**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7EKK6fvIv7Kn"
      },
      "source": [
        "In their paper, Long Short-Term Memory, Sepp Hochreiter & Jürgen Schmidhuber tested an experimental neural network. This new design was an attempt to allow local data to be used and recalled as necessary, extending to lengths it would be implausible to use with earlier designs. They hoped that this new form of neural network would be able to solve complicated time related problems.\n",
        "\n",
        "They explain that Real-Time Recurrent Learning was, by their tests and knowledge, the best previously available network for making decisions based on previously accumulated data. After setting this as a baseline, they claim that RTRL, and similar systems, are dependent on error signals “flowing backwards in time”. However, these signals frequently either exponentially increase, or approach zero. If they blow up, they can cause rapid fluctuations in weight, limiting information that can be gained. If they vanish, they can cause time lags or complete failure to produce a solution.\n",
        "\n",
        "Their proposed architecture is built around a system of memory cells, linked as a singular hidden layer between the input and output layers of the network. A memory cell consists of a linear unit attached to itself, an input gate, an output gate, and differentiable functions to scale messages before they are combined with the messages from these gates. They focus on minimising computational complexity.\n",
        "\n",
        "Through testing, the authors demonstrate that their architecture can outperform other designs in some scenarios. This includes the common benchmark test of embedded Reber grammar (although it is noted that random guesswork can achieve the same result in less time), identifying that the last element of a specially generated random sequence is always identical to the first element, and a problem known as the Temporal Order Problem that serves to highlight the specific advantages this architecture has in ignoring noise to solve long-term arithmetic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6poOOwBKv95D"
      },
      "source": [
        "**Innovation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_J6fwaYv9-8"
      },
      "source": [
        "LSTM architecture has been foundational to the development of recurrent neural networks, and later LSTM networks were applied in Google Translate, Siri, and Amazon Alexa, and other mainstream products. This is because LSTM allows the system to react to the context of data, rather than just the data itself, which was an important breakthrough.\n",
        "\n",
        "They claim their design enforces constant error flow, and prevents the error signal from exploding or vanishing. This is incorrect. Their solution is a bold first step to prevent the error signal vanishing, but it does not prevent either case. Instead, removing the risk of an error signal vanishing is addressed in the addition of a third gate on memory cells known as a “forget gate” in later papers. The issue of the signal exploding is inherent in how the signal is represented. It can be mitigated, which this architecture does, but it cannot be removed. As such, the repeated reference to this idea throughout the paper demonstrates the authors fail to understand the limitations of their own architecture.\n"
      ]
    }
  ]
}