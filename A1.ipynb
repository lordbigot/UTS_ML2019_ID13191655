{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordbigot/UTS_ML2019_ID13191655/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMCFXtnBvs2s",
        "colab_type": "text"
      },
      "source": [
        "**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7EKK6fvIv7Kn"
      },
      "source": [
        "In their paper, Long Short-Term Memory, Sepp Hochreiter & Jürgen Schmidhuber tested an experimental neural network. This new design was an attempt to allow local data to be used and recalled as necessary, extending to lengths it would be implausible to use with earlier designs. They hoped that this new form of neural network would be able to solve complicated time related problems.\n",
        "\n",
        "They explain that Real-Time Recurrent Learning was, by their tests and knowledge, the best previously available network for making decisions based on previously accumulated data. After setting this as a baseline, they claim that RTRL, and similar systems, are dependent on error signals “flowing backwards in time”. However, these signals frequently either exponentially increase, or approach zero. If they blow up, they can cause rapid fluctuations in weight, limiting information that can be gained. If they vanish, they can cause time lags or complete failure to produce a solution.\n",
        "\n",
        "Their proposed architecture is built around a system of memory cells, linked as a singular hidden layer between the input and output layers of the network. A memory cell consists of a linear unit attached to itself, an input gate, an output gate, and differentiable functions to scale messages before they are combined with the messages from these gates. They focus on minimising computational complexity.\n",
        "\n",
        "Through testing, the authors demonstrate that their architecture can outperform other designs in some scenarios. This includes the common benchmark test of embedded Reber grammar (although it is noted that random guesswork can achieve the same result in less time), identifying that the last element of a specially generated random sequence is always identical to the first element, and a problem known as the Temporal Order Problem that serves to highlight the specific advantages this architecture has in ignoring noise to solve long-term arithmetic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6poOOwBKv95D"
      },
      "source": [
        "**Innovation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_J6fwaYv9-8"
      },
      "source": [
        "LSTM architecture has been foundational to the development of recurrent neural networks, and later LSTM networks were applied in Google Translate, Siri, and Amazon Alexa, and other mainstream products. This is because the LSTM architecture allows the system to react to the context of time series data, rather than the individual points of data, which was an important breakthrough. Earlier methods could be adapted to time gaps, but it was infeasible for this to extend over large periods.\n",
        "\n",
        "They claim their design enforces constant error flow, and prevents the error signal from exploding or vanishing. This is incorrect. Their solution is a bold first step to prevent the error signal vanishing, but it does not prevent either case. Instead, removing the risk of an error signal vanishing is addressed in the addition of a third gate on memory cells known as a “forget gate” in later papers. The issue of the signal exploding is inherent in how the signal is represented. It can be mitigated, which this architecture does, but it cannot be removed. As such, the repeated reference to this idea throughout the paper demonstrates the authors fail to understand the limitations of their own architecture.\n",
        "\n",
        "The authors' bias is simple in origin. It serves their interests to play up their achievements, as recognition can open doors in the deep learning industry, as in any other. In numerous places in the article, the authors put forward a pretense that they are discussing flaws and limitations in their system, but turn the argument around, only wishing to discuss its superiority over other systems. If they were to indicate that this was an early build, and that their design failed to complete it's initial goal of eradicating signal exploding or signal vanishing, this could reduce their claim as the designers of LSTM, which they had been working on for almost a decade. Talking down their system could conceal what an achievement this is, and limit their ability to conduct further research in this field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW1tHzmD582n",
        "colab_type": "text"
      },
      "source": [
        "**Technical quality**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUajVImp6LwQ",
        "colab_type": "text"
      },
      "source": [
        "The paper begins by summarising earlier recurrent net architectures. Their objective here is to discredit past work with a veneer of impartiality, however it does also identify good benchmarks with which to compare their algorithm.\n",
        "\n",
        "They correctly derive formulas to prove the existence of exponential increase and decease on error signals as they are passed between any two nodes, and extend this proof over the entire network. Once this formula is derived, it justifies their concept of a Constant Error Carousel, as well as the correct function to use within a neutron.\n",
        "\n",
        "By further exploring this idea, they identify further problems which justify their use of gate units, and thus the architecture of their memory cell. After this is demonstrated, they examine remaining issues to suggest starting conditions that they believe will achieve the best results, and move on to experimentation.\n",
        "\n",
        "In their experiments, they compare their results to results taken by other trials. In doing so, they limit what comparison can be made between LSTM and other systems. For a complete analysis, they should have conducted experiments with an even shorter time lag than Embedded Reber Grammar, to demonstrate the number of time steps required for LSTM to become a more valid solution than other networks.\n",
        "\n",
        "The only direct comparisons between other systems and LSTM come in the first two experiments, 1A and 2. In these experiments, they accumulate sufficient evidence that their system is more effective in a limited number of loops. Their experiment is repeated to minimise the risk of luck. However, they do not address the concern of the time taken to compute each run on identical hardware, which may be a serious concern.\n",
        "\n",
        "In later experiments, they deliberately construct scenarios that work to the algorithm's strengths. These provide useful information about the upper limits of potential offered by the LSTM architecture, however the lack of benchmark given within the addition and multiplication problems limits the usefulness of the data."
      ]
    }
  ]
}