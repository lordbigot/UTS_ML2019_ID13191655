{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordbigot/UTS_ML2019_ID13191655/blob/master/A2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqj6GYRgL1Ap",
        "colab_type": "text"
      },
      "source": [
        "# **Vector-based normal-finding decision tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vaa1Ozd_Q5aP",
        "colab_type": "text"
      },
      "source": [
        "**Algorithm Brief**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCaM3NqCRBFd",
        "colab_type": "text"
      },
      "source": [
        "*Concept*\n",
        "\n",
        "For the creation of a new algorithm, I considered the fundamentals of decision tree classifiers.\n",
        "\n",
        "Decision Tree Classifiers consist of a series of internal nodes, which split the data into smaller and smaller sets. Typically, each decision node evaluates one attribute, in comparison to a target value. Everything below the value falls into a first partition, everything equal to or above the value falls into a second partition.\n",
        "\n",
        "However, it is possible to use more than one attribute in a single step. Consider a linear node, weighing all of the attributes and comparing the result to a static value. Splitting the data set this way reduces the bias that comes from dependence on orthogonal lines to represent the data space. It also significantly increases the complexity of the algorithm, as well as the variance.\n",
        "\n",
        "However, using linear splitting opens unique opportunities. Consider the following example:\n",
        "\n",
        "![alt text](https://i.postimg.cc/GtZy5Hh1/justification.png)\n",
        "\n",
        "In the above diagram, the depicted space is a form of underlying truth. The red space definitively belongs to one class, the yellow space belongs to a different class, and the orange space is disputed. An ideal solution would put lines cleanly through the middle of the orange space.\n",
        "\n",
        "![alt text](https://i.postimg.cc/c1jwH575/justification-1.png)\n",
        "\n",
        "If an orthagonal decision tree algorithm was used, the added line represents a plausible first step. Note that the presence of additional red on the left side of the diagram is likely to weight the line further towards the bottom of the space than an ideal solution.\n",
        "\n",
        "![alt text](https://i.postimg.cc/DyXsbdBf/justification-2.png)\n",
        "\n",
        "This second line completes a good solution. Any additional lines beyond this point would be overfitting the data.\n",
        "\n",
        "![alt text](https://i.postimg.cc/q7jCdmMw/justification-3.png)\n",
        "\n",
        "This diagram shows 2 possible first steps produced by a tree using linear nodes. This data used directly appears to produce worse results than the orthogonal solution. However, the use of lines enables additional calculation. These two lines can be considered an approximation of a curve, and their intersection a point of interest\n",
        "\n",
        "![alt text](https://i.postimg.cc/GpcsV0r9/justification-4.png)\n",
        "\n",
        "The line that has been added represents the tangent to the proposed approximation of a curve.\n",
        "\n",
        "![alt text](https://i.postimg.cc/FHXSbdBy/justification-5.png)\n",
        "\n",
        "The line that has been added represents the normal to the proposed approximation of a curve.\n",
        "\n",
        "![alt text](https://i.postimg.cc/vZ7V0YJp/justification-6.png)\n",
        "\n",
        "If the normal is used to partition the data, instead of either earlier linear node, this will split the data into regions that can be judged more fairly.\n",
        "\n",
        "![alt text](https://i.postimg.cc/Tw152rK6/justification-7.png)\n",
        "\n",
        "This possible result indicates how the use of the normal may limit the influence of unrelated curves upon the final tree, and may limit variance. Note that the depicted scenario has been deliberately chosen to favour orthagonal lines, and yet the linear normal method displays a significant advantage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1TFAiU6pslw",
        "colab_type": "text"
      },
      "source": [
        "*Inputs*\n",
        "\n",
        "This algorithm will be dependent on specific gradients, and so for best results, all data points should be linearly normalised into an n+1-dimensional array with attribute values (up to index n-1) between 0.0 and 1.0, and whatever attribute best represents the class in index n.\n",
        "\n",
        "If, after the training set has been used to find values, the testing set includes results that fall outside the boundaries, the classifier should be able to handle these inputs normally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED_mke81pIPz",
        "colab_type": "text"
      },
      "source": [
        "*Outputs*\n",
        "\n",
        "After training data is input, the output should be in the form of a decision tree. An efficient method of storing this in Python is a list with a length of n+2. The first n values represent multipliers for each attribute of a new data point. If the sum of each attribute, multiplied by its multiplier, is less than 1.0, then it should be sent to the nested tree at index n. If this sum is equal to or greater than 1.0, it should instead be sent to the nested tree at index n+1. This procedure is continued until in the place where a tree would normally be, an object is present, representing the decided class.\n",
        "\n",
        "The output represents the decided class. It is a single value of whatever type the class attributes provided in the training data were. This should not be a list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GdJ38zdwHdg",
        "colab_type": "text"
      },
      "source": [
        "*Intermediate Data Structure*\n",
        "\n",
        "The classifier is best resolved as a recursive function, which must accept its current set as a parameter.\n",
        "\n",
        "The classifier needs to store vectors, and can do so in the format of an n-dimensional array.\n",
        "\n",
        "The classifier must be capable of identifying the proportions of all labels present in the set it has been passed. This information can be stored as 2 parallel 1-dimensional arrays.\n",
        "\n",
        "The classifier must compare gradients. The metric I am using to compare them is derived from entropy, but I have labelled it \"certainty\" to match it's new meaning. a certainty of 1.0 is the best-case scenario, and anything less indicates that the attached gradient cannot divide the data completely in half.\n",
        "\n",
        "The calculation of the \"normal\" in n-dimensions requires an abstract representation of the circle that the two gradients lie between. My solution uses the following formulae as its basis:\n",
        "\n",
        "x = asin(α)\n",
        "\n",
        "y = asin(α+θ)\n",
        "\n",
        "where: x represents an attribute of the first vector; y represents the same attribute of the second vector; a represents the amplitude of the sine wave, the distance between the maximum attribute and 0; α represents the angle on the sine wave at which the first vector is situated, and θ represents the angle between the first and second vector. It's possible that α will be 180° out of phase, and a will be negative, but use of Python 3's atan2() function effectively negate any mistake due to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_zu5u7T3GiR",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzjLm4LO3-dm",
        "colab_type": "text"
      },
      "source": [
        "The skeleton of this classifier, within it's main function, indicates the other requirements. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBAfiUzY4tSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_classifier(training_set):\n",
        "    label_index = len(training_set[0]) - 1\n",
        "    labels, label_frequencies = list_labels(training_set, label_index)\n",
        "    if len(labels) == 1:\n",
        "        return labels[0]\n",
        "    elif len(training_set) < MINIMUM_LENGTH:\n",
        "        max_index = 0\n",
        "        for i in range(1, len(labels)):\n",
        "            if label_frequencies[max_index] < label_frequencies[i]:\n",
        "                max_index = i\n",
        "        return labels[max_index]\n",
        "    for i in range(len(labels)):\n",
        "        if label_frequencies[i] > len(training_set) * MINIMUM_PROPORTION:\n",
        "            return labels[i]\n",
        "    gradients = gradient_set\n",
        "    most_certain_gradients = maximum_certainty(training_set, gradients, label_index)\n",
        "    best_gradient = gradients[most_certain_gradients[0][0]]\n",
        "    second_gradient = gradients[most_certain_gradients[1][0]]\n",
        "    best_gradient_certainty = most_certain_gradients[0][1]\n",
        "    second_gradient_certainty = most_certain_gradients[1][1]\n",
        "    if best_gradient_certainty < 1.0 and NORMAL_FACTOR * second_gradient_certainty >= best_gradient_certainty:\n",
        "        modulo_one = vector_modulo(best_gradient)\n",
        "        for i in range(len(best_gradient)):\n",
        "            best_gradient[i] /= modulo_one\n",
        "        modulo_two = vector_modulo(second_gradient)\n",
        "        for i in range(len(second_gradient)):\n",
        "            second_gradient[i] /= modulo_two\n",
        "        theta = math.acos(numpy.dot(best_gradient, second_gradient))\n",
        "        alpha = []\n",
        "        for i in range(len(best_gradient)):\n",
        "            alpha.append(\n",
        "                math.atan2(best_gradient[i] * math.sin(theta),\n",
        "                           second_gradient[i] - best_gradient[i] * math.cos(theta)))\n",
        "        a = []\n",
        "        for i in range(len(best_gradient)):\n",
        "            if math.sin(alpha[i]) == 0:\n",
        "                a.append(second_gradient[i] / math.sin(alpha[i] + theta))\n",
        "                continue\n",
        "            a.append(best_gradient[i] / math.sin(alpha[i]))\n",
        "        new_gradient = []\n",
        "        for i in range(len(best_gradient)):\n",
        "            new_gradient.append(a[i] * math.sin(alpha[i] + theta / 2.0 + math.pi / 2.0))\n",
        "        sorted_set = sort_list_by_proportion(training_set, new_gradient)\n",
        "        best_certainty = 0\n",
        "        best_index = 0\n",
        "        for i in range(2, len(sorted_set) - 1):\n",
        "            split_set = sort_list_by_proportion(sorted_set[:i], best_gradient)\n",
        "            leftover_set = sort_list_by_proportion(sorted_set[i:], second_gradient)\n",
        "            best = max(grade_list_by_certainty(split_set, label_index)) + \\\n",
        "                   max(grade_list_by_certainty(leftover_set, label_index))\n",
        "            if best_certainty < best:\n",
        "                best_certainty = best\n",
        "                best_index = i\n",
        "        for i in range(2, len(sorted_set) - 1):\n",
        "            split_set = sort_list_by_proportion(sorted_set[:i], second_gradient)\n",
        "            leftover_set = sort_list_by_proportion(sorted_set[i:], best_gradient)\n",
        "            best = max(grade_list_by_certainty(split_set, label_index)) + \\\n",
        "                   max(grade_list_by_certainty(leftover_set, label_index))\n",
        "            if best_certainty < best:\n",
        "                best_certainty = best\n",
        "                best_index = i\n",
        "        sum_value = 0\n",
        "        for i in range(label_index):\n",
        "            sum_value += (sorted_set[best_index][i] + sorted_set[best_index + 1][i]) / 2.0 * new_gradient[i]\n",
        "        if sum_value < 0:\n",
        "            sum_value *= -1\n",
        "        for i in range(label_index):\n",
        "            new_gradient[i] /= sum_value\n",
        "        result = new_gradient\n",
        "        result.append(next_classifier(sorted_set[:best_index]))\n",
        "        result.append(next_classifier(sorted_set[best_index:]))\n",
        "        return result\n",
        "    elif best_gradient_certainty == 1.0:\n",
        "        sorted_set = sort_list_by_proportion(training_set, best_gradient)\n",
        "        first_index = 0\n",
        "        second_index = len(sorted_set) - 1\n",
        "        first_class = sorted_set[first_index][label_index]\n",
        "        second_class = sorted_set[second_index][label_index]\n",
        "        while first_index + 1 < second_index:\n",
        "            third_index = (first_index + second_index) // 2\n",
        "            third_class = sorted_set[third_index][label_index]\n",
        "            if third_class == first_class:\n",
        "                first_index = third_index\n",
        "            else:\n",
        "                second_index = third_index\n",
        "        barrier_sum = 0.0\n",
        "        for i in range(label_index):\n",
        "            barrier_sum += (sorted_set[first_index][i] + sorted_set[second_index][i]) / 2.0 * best_gradient[i]\n",
        "        if barrier_sum < 0:\n",
        "            barrier_sum *= -1\n",
        "        result = []\n",
        "        for i in range(label_index):\n",
        "            result.append(best_gradient[i] / barrier_sum)\n",
        "        result.append(first_class)\n",
        "        result.append(second_class)\n",
        "        return result\n",
        "    sorted_set = sort_list_by_proportion(training_set, best_gradient)\n",
        "    best_certainty = 0\n",
        "    best_index = 0\n",
        "    for i in range(2, len(sorted_set) - 1):\n",
        "        split_set = sorted_set[:i]\n",
        "        leftover_set = sorted_set[i:]\n",
        "        best = max(grade_list_by_certainty(split_set, label_index)) + \\\n",
        "               max(grade_list_by_certainty(leftover_set, label_index))\n",
        "        if best_certainty < best:\n",
        "            best_certainty = best\n",
        "            best_index = i\n",
        "    sum_value = 0\n",
        "    for i in range(label_index):\n",
        "        sum_value += (sorted_set[best_index][i] + sorted_set[best_index + 1][i]) / 2.0 * best_gradient[i]\n",
        "    if sum_value < 0:\n",
        "        sum_value *= -1\n",
        "    for i in range(label_index):\n",
        "        best_gradient[i] /= sum_value\n",
        "    result = best_gradient\n",
        "    result.append(next_classifier(sorted_set[:best_index]))\n",
        "    result.append(next_classifier(sorted_set[best_index:]))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACQt8RUo6j3U",
        "colab_type": "text"
      },
      "source": [
        "The classifier requires the use of a set of gradients. My early attempts to produce this based on the data were slow and ineffectual, so the algorithm now uses a consistent list, generated each time a new list is loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSAN8wa9-CZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradient_set = []\n",
        "\n",
        "\n",
        "def initialize_gradient_set(n):\n",
        "    lesser_set = []\n",
        "    if n > 1:\n",
        "        lesser_set = initialize_gradient_set(n - 1)\n",
        "    else:\n",
        "        return [[1]]\n",
        "    result = []\n",
        "    for i in range(len(lesser_set)):\n",
        "        for j in range(n):\n",
        "            result.append(lesser_set[i][:j] + [0] + lesser_set[i][j:])\n",
        "        maximum_value = 0\n",
        "        for j in range(n - 1):\n",
        "            if lesser_set[i][j] > maximum_value:\n",
        "                maximum_value = lesser_set[i][j]\n",
        "        print(\"max_value:\", maximum_value)\n",
        "        max_index = int(math.log2(maximum_value))\n",
        "        print(\"max_index:\", max_index)\n",
        "        for j in range(n):\n",
        "            for k in range(max_index + 2):\n",
        "                new_set = lesser_set[i][:j] + [int(math.pow(2, k))] + lesser_set[i][j:]\n",
        "                breaking = False\n",
        "                for old_set in result:\n",
        "                    if new_set == old_set:\n",
        "                        breaking = True\n",
        "                        break\n",
        "                if breaking:\n",
        "                    continue\n",
        "                result.append(new_set)\n",
        "            for k in range(max_index + 2):\n",
        "                new_set = lesser_set[i][:j] + [-int(math.pow(2, k))] + lesser_set[i][j:]\n",
        "                if new_set[0] < 0:\n",
        "                    for l in range(len(new_set)):\n",
        "                        new_set[l] *= -1\n",
        "                breaking = False\n",
        "                for old_set in result:\n",
        "                    if new_set == old_set:\n",
        "                        breaking = True\n",
        "                        break\n",
        "                if breaking:\n",
        "                    continue\n",
        "                result.append(new_set)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9aJTE5BOs6",
        "colab_type": "text"
      },
      "source": [
        "The classifier needs a function to sort a list by any vector array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPL7XvY_CRRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_list_by_proportion(data, proportions):\n",
        "    new_list = data.copy()\n",
        "    editable_length = len(new_list)\n",
        "    while editable_length > 1:\n",
        "        max_index = 0\n",
        "        maximum_value = value_vector_by_proportions(new_list[0], proportions)\n",
        "        for i in range(1, editable_length):\n",
        "            current_value = value_vector_by_proportions(new_list[i], proportions)\n",
        "            if current_value > maximum_value:\n",
        "                max_index = i\n",
        "                maximum_value = current_value\n",
        "        editable_length -= 1\n",
        "        new_list[editable_length], new_list[max_index] = new_list[max_index], new_list[editable_length]\n",
        "    return new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVFkMyKL_Z7L",
        "colab_type": "text"
      },
      "source": [
        "Entropy calculations all use a basic formula to combine the bits of information with the size of the known information.\n",
        "\n",
        "The sum of the entropy of the proportions of a set can be used to determine how uniform the set is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suW8OIJZC6q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(probability):\n",
        "    return probability * math.log2(probability)\n",
        "\n",
        "\n",
        "def list_entropy(data, label_index):\n",
        "    proportions = class_proportions(data, label_index)\n",
        "    result = 0.0\n",
        "    for proportion in proportions:\n",
        "        result -= entropy(proportion)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVvbE0GAFZsS",
        "colab_type": "text"
      },
      "source": [
        "Effective use of entropy is dependent on proper formulae to deal with lists.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdTXQIBbI-G5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_proportions(data, label_index):\n",
        "    labels = []\n",
        "    label_frequencies = []\n",
        "    for item in data:\n",
        "        found_label = False\n",
        "        for i in range(len(labels)):\n",
        "            if item[label_index] == labels[i]:\n",
        "                found_label = True\n",
        "                label_frequencies[i] += 1\n",
        "        if not found_label:\n",
        "            labels.append(item[label_index])\n",
        "            label_frequencies.append(1)\n",
        "    result = []\n",
        "    for frequency in label_frequencies:\n",
        "        result.append(float(frequency) / float(len(data)))\n",
        "    return result\n",
        "\n",
        "\n",
        "def grade_list_by_certainty(data, label_index):\n",
        "    new_list = []\n",
        "    for i in range(1, len(data)):\n",
        "        proportion_before_i = float(i) / len(data)\n",
        "        proportion_after_i = 1.0 - proportion_before_i\n",
        "        new_list.append(1.0 - ((list_entropy(data[:i], label_index) * proportion_before_i)\n",
        "                               + (list_entropy(data[i:], label_index) * proportion_after_i)))\n",
        "    return new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGBhGF3NLNO9",
        "colab_type": "text"
      },
      "source": [
        "The ability to evaulate the certainty of a list is useful in greating a fuly informed comparison of gradients, and finding which can produce a complete result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB8dnFpfLOar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maximum_certainty(data, gradients, label_index):\n",
        "    print(\"Flag MC-1\")\n",
        "    print(\"gradients:\", gradients)\n",
        "    certainties = [[0, -1.0]]\n",
        "    for h in range(len(gradients)):\n",
        "        sorted_data = sort_list_by_proportion(data, gradients[h])\n",
        "        marks = grade_list_by_certainty(sorted_data, label_index)\n",
        "        for i in range(len(marks)):\n",
        "            breaking = False\n",
        "            for certainty_index in range(len(certainties)):\n",
        "                if certainties[certainty_index][1] < marks[i]:\n",
        "                    if certainties[certainty_index][0] != h:\n",
        "                        certainties.insert(certainty_index, [h, marks[i]])\n",
        "                    else:\n",
        "                        certainties[certainty_index][0] = h\n",
        "                        certainties[certainty_index][1] = marks[i]\n",
        "                    breaking = True\n",
        "                    break\n",
        "                if certainties[certainty_index][0] == h:\n",
        "                    breaking = True\n",
        "                    break\n",
        "            if not breaking:\n",
        "                certainties.append([h, marks[i]])\n",
        "    return certainties"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sVrzJBBODum",
        "colab_type": "text"
      },
      "source": [
        "Calculating the complete distance of a vector early on is necessary for the creation a unit circle, necessary to calculate the normal. Luckily, this is a very simple formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8Z1Ium8Qa75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_modulo(vector):\n",
        "    return pow(sum(pow(i, 2) for i in vector), 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-v9GOnkQb8b",
        "colab_type": "text"
      },
      "source": [
        "Several of the features of the classifier are dependent on knowing the proportions of the classes present in the set. In fact, if the proportion of one label within the set passes over a certain threshold, the classifier will be considered complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v80p2OrRxSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_labels(data, label_index):\n",
        "    labels = []\n",
        "    label_frequencies = []\n",
        "    for item in data:\n",
        "        found_label = False\n",
        "        for i in range(len(labels)):\n",
        "            if item[label_index] == labels[i]:\n",
        "                found_label = True\n",
        "                label_frequencies[i] += 1\n",
        "        if not found_label:\n",
        "            labels.append(item[label_index])\n",
        "            label_frequencies.append(1)\n",
        "    return [labels, label_frequencies]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNO5oA4GSwFm",
        "colab_type": "text"
      },
      "source": [
        "Some constants are used in the above formulae. Here's what they represent:\n",
        "\n",
        "The normal factor is the ratio of how close the certainty value of two possible vector nodes must be for finding a normal to be considered a good course of action, over just directly using the better vector.\n",
        "\n",
        "The minimum length is the minimum number of data points that the algorithm is willing to split, instead of just assigning the best available value. The maximum proportion is the point at which a value is considered effectively representative of the whole set. Both of these limits exist to prevent overfitting, but they may need to be altered depending on the specific nature of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e_2YuouS4NB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NORMAL_FACTOR = 1.2\n",
        "MINIMUM_LENGTH = 4\n",
        "MAXIMUM_PROPORTION = 0.95"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUuZ2agVRyOI",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk0CslCHSqHI",
        "colab_type": "text"
      },
      "source": [
        "The following code tests the accuracy of the above algorithm upon a set of data randomly selected to match a known underlying truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCSxkO19StGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def diamond_test():\n",
        "    global gradient_set\n",
        "    gradient_set = initialize_gradient_set(2)\n",
        "    training_data = []\n",
        "    import random\n",
        "    view = []\n",
        "    for i in range(21):\n",
        "        view.append(\"                     \")\n",
        "    for i in range(300):\n",
        "        point = [random.random(), random.random()]\n",
        "        if point[0] * 2 + point[1] > 1.5 and point[0] - point[1] * 2 < -0.5:\n",
        "            point.append(1)\n",
        "            view[math.floor((point[0] + 0.025) / 0.05)] = view[math.floor((point[0] + 0.025) / 0.05)] \\\n",
        "                                                              [:math.floor((point[1] + 0.025) / 0.05)] + \"1\" \\\n",
        "                                                          + view[math.floor((point[0] + 0.025) / 0.05)][\n",
        "                                                            math.floor((point[1] + 0.025) / 0.05) + 1:]\n",
        "        else:\n",
        "            point.append(0)\n",
        "            view[math.floor((point[0] + 0.025) / 0.05)] = view[math.floor((point[0] + 0.025) / 0.05)][\n",
        "                                                          :math.floor((point[1] + 0.025) / 0.05)] + \"0\" + view[\n",
        "                                                                                                              math.floor(\n",
        "                                                                                                                  (\n",
        "                                                                                                                          point[\n",
        "                                                                                                                              0] + 0.025) / 0.05)][\n",
        "                                                                                                          math.floor((\n",
        "                                                                                                                             point[\n",
        "                                                                                                                                 1] + 0.025) / 0.05) + 1:]\n",
        "        training_data.append(point)\n",
        "    classifier = next_classifier(training_data)\n",
        "    for i in range(21):\n",
        "        print(view[i])\n",
        "    print(classifier)\n",
        "    for i in range(21):\n",
        "        line = \"\"\n",
        "        for j in range(21):\n",
        "            point = [i / 20.0, j / 20.0]\n",
        "            line += str(classify_vector(point, classifier))\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5TpEjhWMRF",
        "colab_type": "text"
      },
      "source": [
        "Executing this test repeatedly in a development environment I produced the following results:\n",
        "\n",
        "The following is the underlying truth, the ideal result for all other results to be compared to:\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000011\n",
        "\n",
        "000000000000000001111\n",
        "\n",
        "000000000000000111111\n",
        "\n",
        "000000000000011111111\n",
        "\n",
        "000000000001111111111\n",
        "\n",
        "000000000001111111111\n",
        "\n",
        "000000000001111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000011111111\n",
        "\n",
        "000000000000011111111\n",
        "\n",
        "000000000000001111111\n",
        "\n",
        "000000000000001111111\n",
        "\n",
        "000000000000000111111\n",
        "\n",
        "000000000000000111111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "The following depict the appoximations produced by the algorithm:\n",
        "\n",
        "[2.54432421402722, -1.8338451360721781, [8.828184911424573, -12.248452278886026, [-5.797405599084514, 8.043470604001524, [0.0, 1.7599002934694448, 0, 1], [1.333766989884266, 0.666883494942133, 0, 1]], 0], 0]\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000001\n",
        "\n",
        "000000000000000000111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000001111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000011111111111\n",
        "\n",
        "000000000001111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000011111111\n",
        "\n",
        "000000000000000111111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000001111\n",
        "\n",
        "[2.2296978512073298, -1.607075284197018, [5.610344594580424, -7.7839373239561755, [1.3347194210343847, 0.6673597105171923, 0, 1], 0], 0]\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000001\n",
        "\n",
        "000000000000000000111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000001111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000011111111111\n",
        "\n",
        "000000001111111111111\n",
        "\n",
        "000000011111111111111\n",
        "\n",
        "000000011111111111111\n",
        "\n",
        "000000001111111111111\n",
        "\n",
        "000000000111111111111\n",
        "\n",
        "000000000011111111111\n",
        "\n",
        "000000000000111111111\n",
        "\n",
        "000000000000011111111\n",
        "\n",
        "000000000000001111111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "[3.330385031667594, -2.4004057179113363, [-6.309592089865182, 8.754091400140329, 0, [1.3342375400368536, 0.6671187700184268, 0, 1]], [1.8663649701339, -3.7327299402678, 1, 0]]\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000001\n",
        "\n",
        "000000000000000000111\n",
        "\n",
        "110000000000000011111\n",
        "\n",
        "111000000000001111111\n",
        "\n",
        "111110000000111111111\n",
        "\n",
        "111111000011111111111\n",
        "\n",
        "011111100001111111111\n",
        "\n",
        "011111111001111111111\n",
        "\n",
        "001111111100111111111\n",
        "\n",
        "001111111111011111111\n",
        "\n",
        "000111111111101111111\n",
        "\n",
        "000111111111111111111\n",
        "\n",
        "000011111111111111111\n",
        "\n",
        "000011111111111111111\n",
        "\n",
        "000001111111111111111\n",
        "\n",
        "000001111111111111111\n",
        "\n",
        "[-9.765671333647328, 2.3053622806618006, [0.36891361632398023, -1.562743156573656, [0.0, 1.288140391703908, 0, 1], [0.42444317709396595, -1.797970150756021, [0.0, 1.4426725051496716, 0, 1], [-0.461127511154061, 1.9533674835438966, 0, [2.416857374116822, 0.0, 0, 1]]]], 0]\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000000000\n",
        "\n",
        "000000000000000011000\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111\n",
        "\n",
        "000000000000000011111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv_KV3tGcJ5T",
        "colab_type": "text"
      },
      "source": [
        "As this test clearly demonstrates, this algorithm is still prone to flaws, but can produce some very accurate comparisons. Although it is in need of further refinement, it can successfully produce some accurate approximations.\n",
        "\n",
        "However, the execution time is substantially lacking. This test, on a set of 200 2-dimensional data points, took several minutes to produce a single classifier. The time increase due to raising the number of data points may occur on a polynomial scale, and raising the dimensions on an exponential scale.\n",
        "\n",
        "This raises concerning issues, as discussed in the Conclusion "
      ]
    }
  ]
}